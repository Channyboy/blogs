---
layout: post
title: "Forwarding Open Liberty logs to external cluster using OCP Log Forwarding API"
categories: blog
author_picture: https://avatars3.githubusercontent.com/leemelim
author_github: https://github.com/leemelim
seo-title: Forwarding Open Liberty logs to external cluster using OCP Log Forwarding API - OpenLiberty.io
seo-description: Using Log Forwarding API, you can send logs from Open Liberty deployed in OpenShift Container Platform to remote destinations. Logs, which get collected by cluster logging in OpenShift Container Platform, can be forwarded to either external Elasticsearch cluster or external log aggregation solution using forward protocol.
blog_description: Using Log Forwarding API, you can send logs from Open Liberty deployed in OpenShift Container Platform to remote destinations. Logs, which get collected by cluster logging in OpenShift Container Platform, can be forwarded to either external Elasticsearch cluster or external log aggregation solution using forward protocol.
---
= Forwarding Open Liberty logs to external cluster using OCP Log Forwarding API
Halim Lee <https://github.com/leemelim>

Container orchestration system is highly distributed and its parts are dynamic. It becomes difficult to detect which worker nodes are running your containers and pods for your application. Application logging and log collection helps you monitor and troubleshoot these applications efficiently. Openshift Container Platform provides a log aggregation solution with cluster logging. The cluster logging is consisted of three components: Elasticsearch, Fluentd, and Kibana (EFK). Fluentd is deployed to each node in the OpenShift Container Platform cluster and forwards all logs to Elasticsearch. Elasticsearch stores and organizes the log data. Kibana helps you create visualizations and searches with the aggregated data.

Starting OpenShift 4.3, you can use Log Forwarding API to send logs to external sources. It makes it possible for you to use external log analysis tools for Open Liberty running on your OpenShift cluster. The Log Forwarding API enables you to configure custom pipelines to send logs to specific endpoints within or outside of your OpenShift cluster. 

image::/img/blog/log-forwarding.png[Log Forwarding,width=70%,align="center"]

== Configuring Log Forwarding API

. Ensure your cluster logging instance is created and all pods are fully operational. See the installation and configuration guide for cluster logging: link:https://docs.openshift.com/container-platform/4.4/logging/cluster-logging-deploying.html[Deploying cluster logging].

. Create a Log Forwarding instance object YAML file, `log-forward-instance.yaml`. Configure outputs and pipelines.
* Outputs can be either `elasticsearch` or `forward`. `elasticsearch` routes logs to internal or external Elasticsearch cluster. `forward` forwards logs to an external log aggregation solution and this option uses the Fluentd forward protocols.
* Pipelines can be one of `logs.app`, `logs.infra` and `logs.audit`. `logs.app` are container logs generated by user applications. `logs.infra` are logs generated by infrastructure components running in the cluster and OpenShift Container Platform nodes. `logs.audit` are logs generated by the node audit system, which are stored in the /var/log/audit/audit.log file.
* Sample log forwarding instance configuration:
+
```
apiVersion: "logging.openshift.io/v1alpha1"
kind: "LogForwarding"
metadata:
  name: instance 
  namespace: openshift-logging
spec:
  disableDefaultForwarding: true 
  outputs: 
   - name: elasticsearch 
     type: "elasticsearch"  
     endpoint: elasticsearch.openshift-logging.svc:9200 
     secret: 
        name: fluentd
   - name: elasticsearch-insecure 
     type: "elasticsearch"  
     endpoint: http://elasticsearch-insecure.offcluster.com:9200 
     insecure: true
   - name: fluentd-forward
     type: "forward"
     endpoint: https://fluentd-forward.svc:24224
     secret:
        name: fluentd-forward
  pipelines: 
   - name: container-logs 
     inputSource: logs.app 
     outputRefs: 
     - elasticsearch
     - elasticsearch-insecure
     - fluentd-forward
   - name: infra-logs
     inputSource: logs.infra
     outputRefs:
     - elasticsearch
   - name: audit-logs
     inputSource: logs.audit
     outputRefs:
     - fluentd-forward
```
+
The example has three outputs defined, elasticsearch routing to internal Elasticsearch instance, elasticsearch routing to external insecure Elasticsearch cluster and forward routing to an instance of Fluentd. Each log type is defined under pipelines with its configured ouputs.

. Create the instance:
+
[source]
----
[root@ocp ~]# oc create -f log-forward-instance.yaml
----
+

. Liberty application pods output logs in JSON format, therefore it is recommended to set Fluentd to parse the JSON fields from the message body. This feature is disabled by default. To enable it, set the cluster logging instance's *managementState* field from *"Managed"* to *"Unmanaged"*.
+
```
[root@ocp ~]# oc edit ClusterLogging instance

apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogging"
metadata:
  name: "instance"

....

spec:
  managementState: "Unmanaged"
```
+
Then, set the environment variable *MERGE_JSON_LOG* to *true*.
+
[source]
----
[root@ocp ~]# oc set env ds/fluentd MERGE_JSON_LOG=true
----
+

. To check if the logs are being forwarded to the specified outputs, run the following command:
+
[source]
----
[root@ocp ~]# oc -n openshift-logging get cm fluentd -o json | jq -r '.data."fluent.conf"' > fluentd-with-logfowarding.conf
----
+
Check if the outputs are defined inside the configuration file.

* For example:
+
```
...
<label @CONTAINER_LOGS>
  <match **>
    @type copy

    <store>
      @type relabel
      @label @ELASTICSEARCH
    </store>
    <store>
      @type relabel
      @label @ELASTICSEARCH-INSECURE
    </store>
    <store>
      @type relabel
      @label @FLUENTD-FORWARD
    </store>
  </match>
</label>
...
```
+


== Splunk configuration on external cluster

. Download and Install the following tools on your external machine:
* Git
* OpenShift Command Line Tool
* Helm Command Line Tool

. Login to OpenShift through command line tool with a user with `cluster-admin` permissions.

. Clone the git repository to your machine:
+
[source]
----
[root@ocp ~]# git clone https://github.com/sabre1041/openshift-logforwarding-splunk.git
[root@ocp ~]# cd openshift-logforwarding-splunk
----
+

. Deploy a nonpersistent instance of splunk to a project called `splunk`.
+
[source]
----
[root@ocp ~]# ./splunk-install.sh
----
+
Once splunk is deployed, you can login by discovering the exposed link.
+
[source]
----
[root@ocp ~]# echo https://$(oc get routes -n splunk splunk -o jsonpath='{.spec.host}')
----
+
The default credentials are:
+
```
Username: admin
Password: admin123
```
+

. Go to Search & Reporting and search for `index="openshift"` to view logs from OpenShift Container Platform.

. Download Splunk dashboards for Open Liberty: link:https://github.com/WASdev/sample.dashboards/tree/master/Liberty/Splunk%208[Sample dashboard for Liberty using Splunk]. Import downloaded sample dashboards using *Source* option. Using this dashboard, you can visualize message, trace, and first failure data capture (FFDC) logging data collected from JSON logging in Open Liberty.

image::/img/blog/splunk-dashboard.png[Splunk-Dashboard,width=70%,align="center"]

== External Elasticsearch and Kibana configuration
Using `elasticsearch` output, you can forward logs to your external Elasticsearch cluster.

. Create docker-compose.yaml file for Elasticsearch and Kibana deployment in your external machine.
* Sample docker-compose configuration:
+
```
version: '3'
services:

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:5.6.16
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - ./elasticsearch/data:/usr/share/elasticsearch/data

  kibana:
    image: docker.elastic.co/kibana/kibana:5.6.16
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
```
+
Deploy Elasticsearch and Kibana.
+
[source]
----
[root@ocp ~]# docker-compose up
----
+

. Download Kibana dashboards for Open Liberty: link:https://github.com/OpenLiberty/open-liberty-operator/tree/master/deploy/dashboards/logging[Sample Kibana dashboards for Open Liberty]. Import downloaded sample dashboards in Kibana Management > Saved Objects.

image::/img/blog/kibana-dashboard.png[Kibana Dashboard,width=70%,align="center"]

== Conclusion
Application logging is one of the fundamental types of observability. It helps you monitor and find the problems on your servers easily. Using Log Forwarding API, you can use existing external enterprise log collection solutions for OpenShift Container Platform logs. We have now seen two popular log collection solutions: Splunk and Kibana. Both applications allow you to aggregate and analyze your Open Liberty log events on OpenShift Cluster Platform and determine how healthy and well-performing your Liberty servers are. 