---
layout: post
title: "Forwarding Open Liberty logs in OpenShift Container Platform to Splunk using Log Forwarding API"
categories: blog
author_picture: https://avatars3.githubusercontent.com/leemelim
author_github: https://github.com/leemelim
seo-title: Forwarding Open Liberty logs in OpenShift Container Platform to Splunk using Log Forwarding API - OpenLiberty.io
seo-description: Using Log Forwarding API, you can send logs from Open Liberty deployed in OpenShift Container Platform to remote destinations. You can deploy Splunk on your external machine or inside your OpenShift Container Platform. The logs can be forwarded to Splunk using the Fluentd forward protocol.
blog_description: Using Log Forwarding API, you can send logs from Open Liberty deployed in OpenShift Container Platform to remote destinations. You can deploy Splunk on your external machine or inside your OpenShift Container Platform. The logs can be forwarded to Splunk using the Fluentd forward protocol.
---
= Forwarding Open Liberty logs in OpenShift Container Platform to Splunk using Log Forwarding API
Halim Lee <https://github.com/leemelim>

OpenShift Container Platform provides a log aggregation solution with cluster logging using Elasticsearch, Fluentd, and Kibana (EFK stack). In addition to EFK stack, integration with other logging applications has been made available. Starting OpenShift 4.3, you can use Log Forwarding API, which is currently in Technology preview, to send logs from Open Liberty in OpenShift cluster to other available log analysis solutions.

Splunk is one of the most popular logging solutions. If you already have Splunk deployed on your external machine, or if you want to start using Splunk as your new logging solution, you will have to integrate OpenShift with Splunk. Collecting logs in one location helps you monitor all of your applications efficiently, especially because container orchestration system is highly distributed and dynamic. Log Forwarding API simplifies the integration ability of OpenShift cluster with Splunk. You can configure custom pipelines to send logs to desired endpoints within or outside of your OpenShift cluster. 

image::/img/blog/log-forwarding.png[Log Forwarding,width=70%,align="center"]

== Configuring Log Forwarding API

. Ensure your cluster logging instance is created and all pods are fully operational. See the installation and configuration guide for cluster logging: link:https://docs.openshift.com/container-platform/4.4/logging/cluster-logging-deploying.html[Deploying cluster logging].

. Create a Log Forwarding instance object YAML file, `log-forward-instance.yaml`. Configure outputs and pipelines.
* Outputs can be either `elasticsearch` or `forward`. `elasticsearch` routes logs to internal or external Elasticsearch cluster. `forward` forwards logs to an external log aggregation solution through Fluentd forward protocols.
* Pipelines can be one of `logs.app`, `logs.infra` and `logs.audit`. Details on the pipelines can be found here: link:https://docs.openshift.com/container-platform/4.5/logging/cluster-logging-external.html#cluster-logging-collector-log-forward-about_cluster-logging-external[Understanding the Log Forwarding API].
* Sample `log-forward-instance.yaml`:
+
```
apiVersion: "logging.openshift.io/v1alpha1"
kind: "LogForwarding"
metadata:
  name: instance 
  namespace: openshift-logging
spec:
  disableDefaultForwarding: true 
  outputs: 
   - name: elasticsearch 
     type: "elasticsearch"  
     endpoint: elasticsearch.openshift-logging.svc:9200 
     secret: 
        name: fluentd
   - name: fluentd-forward
     type: "forward"
     endpoint: https://splunk-fluentd-forward.offcluster.com:24224
     secret:
        name: secure-forward
  pipelines: 
   - name: container-logs 
     inputSource: logs.app 
     outputRefs: 
     - elasticsearch
     - fluentd-forward
   - name: infra-logs
     inputSource: logs.infra
     outputRefs:
     - elasticsearch
   - name: audit-logs
     inputSource: logs.audit
     outputRefs:
     - elasticsearch
```
+
The sample configuration file has two outputs defined: `elasticsearch` routing to internal Elasticsearch instance and `forward` routing to an instance of Fluentd. Each log type is defined under pipelines with its configured output references.

. Create the instance inside your OpenShift cluster:
+
[source]
----
[root@ocp ~]# oc create -f log-forward-instance.yaml
----
+

. Annotate the ClusterLogging instance to enable the Log Forwarding API.
+
[source]
----
[root@ocp ~]# oc annotate clusterlogging -n openshift-logging instance clusterlogging.openshift.io/logforwardingtechpreview=enabled
----
+

. Liberty application pods output logs in JSON format, therefore it is recommended to set Fluentd to parse the JSON fields from the message body. To enable it, set the cluster logging instance's *managementState* field from *"Managed"* to *"Unmanaged"*.
+
```
[root@ocp ~]# oc edit ClusterLogging instance

apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogging"
metadata:
  name: "instance"

....

spec:
  managementState: "Unmanaged"
```
+
Then, set the environment variable *MERGE_JSON_LOG* to *true*.
+
[source]
----
[root@ocp ~]# oc set env ds/fluentd MERGE_JSON_LOG=true
----
+

. To check if the logs are being forwarded to the specified outputs, run the following command:
+
[source]
----
[root@ocp ~]# oc -n openshift-logging get cm fluentd -o json | jq -r '.data."fluent.conf"' > fluentd-with-logfowarding.conf
----
+
This command gets ConfigMap configuration for Fluentd inside OpenShift Container Platform. Check if the outputs are defined inside the configuration file.

* For example:
+
```
...
<label @CONTAINER_LOGS>
  <match **>
    @type copy

    <store>
      @type relabel
      @label @ELASTICSEARCH
    </store>
    <store>
      @type relabel
      @label @FLUENTD-FORWARD
    </store>
  </match>
</label>
...
```
+


== Splunk and Fluentd configuration

Using `forward` output, you can forward OpenShift Container Platform logs to Splunk using Fluentd forward protocol between two Fluentd servers. You can setup Splunk inside your OpenShift Cluster or on your external machine.

=== Option 1: Setting up Splunk and Fluentd on your external machine

This option sets up Splunk and Fluentd manually on your external machine. If you already have Splunk deployed on your external machine, this option will help you setup the connection between your OpenShift cluster and Splunk. Along with Splunk, you have to deploy an instance of Fluentd on your machine to receive packets from Fluentd inside your OpenShift cluster. For the setup demo purposes, docker compose will be used for installation and deployment of external Fluentd and Splunk.

. Create `Dockerfile` to install essential packages while building Fluentd docker image. You need to install *build-essential* to install all dependencies and *fluent-plugin-splunk-enterprise* in order to forward the logs to Splunk.
* Sample `Dockerfile`:
+
```
# fluentd/Dockerfile
FROM fluent/fluentd:v1.10-debian
user 0
RUN apt-get update -y
RUN apt-get install build-essential -y
RUN fluent-gem install fluent-plugin-splunk-enterprise -v 0.10.0
```
+

. Create `docker-compose.yaml` file for Fluentd and Splunk deployment on your external machine.
* Sample `docker-compose.yaml`:
+
```
version: '3'

services:
  splunk:
    hostname: splunk
    image: splunk/splunk:latest
    environment:
      SPLUNK_START_ARGS: --accept-license
      SPLUNK_ENABLE_LISTEN: 8088
      SPLUNK_PASSWORD: changeme
    ports:
      - "8000:8000" 
      - "8088:8088"

  fluentd:
    build: ./fluentd
    volumes:
      - ./fluentd/conf:/fluentd/etc
    links:
      - "splunk"
    ports:
      - "24224:24224"
      - "24224:24224/udp"
```
+
Configure the ports for Splunk and Fluentd. You can also define splunk password under *splunk: environment*.

. Deploy Splunk first to generate HTTP Event Collector token for Fluentd.
+
[source]
----
[root@ocp ~]# docker-compose up splunk
----
+

. Follow the instruction on link:https://openliberty.io/blog/2020/05/27/how-to-analyze-open-liberty-logs-with-splunk.html[How to analyze Open Liberty Logs with Splunk] in section, *Configuring the HTTP Event Collector*. Set *Name* as "openshift". Copy the generated token value.

. Create `fluent.conf` file to configure Fluentd.
* Sample `fluent.conf`:
+
```
<source>
  @type forward
  port 24224
  <security>
    self_hostname splunk-fluentd-forward.offcluster.com
    shared_key "<secret_string>"
  </security>
</source>

<match kubernetes.**>
  @type splunk_hec
  host splunk-fluentd-forward.offcluster.com
  port 8088
  token 00000000-0000-0000-0000-000000000000

  default_source openshift

  use_ssl true
  ca_file /path/to/ca.pem

</match>

```
+
*source* directive determines the input sources. It uses *forward* type to accept TCP packets from your OpenShift Container Platform. *shared_key* is used to connect the Fluentd servers using password authentication. *match* directive determines the output destinations. It looks for events with matching tags and uses *splunk_hec* to sends the events to Splunk using HTTP Event Collector. Splunk's *host* and *port* are required. *token* should be replaced by Splunk's generated token. *default_source* sets the value as source metadata. Set *use_ssl* to true to use SSL when connecting to Splunk.

. Create `secure-forward.conf` to use the Fluentd forward protocol.
* Sample `secure-forward.conf`:
+
```
<store>
  @type forward
  <security>
    self_hostname ${hostname}
    shared_key "<secret_string>"
  </security>

  transport tls
  tls_verify_hostname true
  tls_cert_path '/etc/ocp-forward/ca-bundle.crt'

  <server>
    host splunk-fluentd-forward.offcluster.com
    port 24224
  </server>
</store>
```
+
*store* plugin forwards the logs using *forward* type to specified outputs under *server* directive. *shared_key* value should equal to the value in your external Fluentd configuration file. Specify *transport* tls if you wish to enable TLS validation and specify the path to private CA certificate file using *tls_cert_path*.

. Login to your OpenShift through command line tool. Create a ConfigMap named *secure-forward* in the *openshift-logging* namespace from the configuration file:
+
[source]
----
[root@ocp ~]# oc create configmap secure-forward --from-file=secure-forward.conf -n openshift-logging
----
+
If you need to import secrets required for the receiver, also run the following command:
+
[source]
----
[root@ocp ~]# oc create secret generic secure-forward --from-file=<arbitrary-name-of-key1>=<cert-file-from-your-external-machine> --from-literal=shared_key="<secret_string>"
----
+

. Refresh the *fluentd* Pods to apply the secure-forward secret and secure-forward ConfigMap:
+
[source]
----
oc delete pod --selector logging-infra=fluentd
----
+


=== Option 2: Deploying Splunk onto your OpenShift cluster

This option deploys an instance of Splunk inside your OpenShift cluster using a script. If you do not have Splunk deployed already, this option will make Splunk setup simple through a usage of pre-created configurations. Follow the instructions in OpenShift blog post: link:https://www.openshift.com/blog/forwarding-logs-to-splunk-using-the-openshift-log-forwarding-api[Forwarding Logs to Splunk Using the OpenShift Log Forwarding API] for Splunk setup.

=== Setting up Splunk Dashboard

. Go to Search & Reporting. Search for `source="openshift"` for *Option 1* and `index="openshift"` for *Option 2* to view logs from OpenShift Container Platform.

. Download Splunk dashboards for Open Liberty: link:https://github.com/WASdev/sample.dashboards/tree/master/Liberty/Splunk%208[Sample dashboard for Liberty using Splunk]. Import downloaded sample dashboards using *Source* option. Using this dashboard, you can visualize message, trace, and first failure data capture (FFDC) logging data collected from JSON logging in Open Liberty.

image::/img/blog/splunk-dashboard.png[Splunk-Dashboard,width=70%,align="center"]

== Conclusion
Application logging is one of the fundamental part of application managements. It helps you retrieve and analyze the problems on your servers easily. Using Log Forwarding API, you can use existing external enterprise log collection solutions for OpenShift Container Platform logs. We have now seen a popular log collection solution, Splunk connected with Fluentd. Splunk allows you to aggregate and analyze log events from Open Liberty servers running on OpenShift Container Platform.